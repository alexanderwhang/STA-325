---
title: "Clustering, Mixture Models, and the EM Algorithm"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
editor_options: 
  markdown: 
    wrap: 72
---

# Agenda

-   Clustering
-   Illustrations
-   Two Component Mixture Model
-   Generalized Mixture Model 
-   EM Algorithm

# Clustering

Clustering is an **unsupervised method** that divides up data into
groups (clusters), so that points in any one group are more
\`\`similar'' to each other than to points outside the group

# Clustering methods (that we have covered)

-   Fuzzy clustering (such as deterministic entity resolution or
    blocking)
-   Overlapping clustering (such as locality sensitive hashing)
-   Fellegi Sunter method (technically a mixture model, stay tuned)

# Clustering methods (that we have not covered)

-   Mixture models
-   K-means
-   Hierarchical clustering
-   Others

# Application areas

-   Clustering temperatures to identify weather patterns, grouping
    individuals based on height and weight similarities.
-   Clustering customers based on satisfaction levels (ordinal) or
    grouping individuals based on gender (categorical).
-   Clustering GPS coordinates to identify spatial patterns, grouping
    locations on a map based on features.
-   Clustering users in a social network or data based based on their
    connections (edge structure), grouping academic papers based on
    citation patterns.

<!-- Application areas -->

<!-- === -->

<!-- - Clustering articles based on topics, grouping emails by subject matter. -->

<!-- - Clustering photographs by content, grouping medical images based on visual features. -->

<!-- - Clustering music tracks by genre, grouping speech recordings by language. -->

<!-- - Clustering surveillance footage to identify similar activities, grouping video clips by visual similarities. -->

# History

-   First proposed by Karl Pearson (1984) and analyzed on crab data.
-   Applications: "agriculture, astronomy, bioinformatics, biology,
    economics, engineering, genetics, imaging, marketing, medicine,
    neuroscience, psychiatry, and psychology, among many other fields in
    the biological, physical, and social sciences". McLachlan et. al
    (2019).
-   One of the methods in machine learning is **topic modeling**, which
    identifies "topics" in collections of documents/webpages.
-   Topic modeling relies on mixtures models.

# Motivation

-   Suppose we want to simulate the price of a randomly chosen book.

-   Paperbacks are often cheaper than hardbacks, so let's model them
    separately.

-   Model the price of a book as a mixture model.

-   There will be two components (or clusters) in our model -- one for
    paperbacks and one for hardbacks.

# Model

-   Paperback distribution: $N(9, 1)$
-   Hardback distribution: $N(20, 1)$
-   Assume that there's a there is a 50% chance of choosing a paperback
    and 50% of choosing hardback.

# Motivation

```{r, echo=FALSE}
numSamples <- 5000
prices      <- numeric(numSamples)
for(i in seq_len(numSamples)) {
  # draw latent component
  z.i <- rbinom(1,1,0.5)
  # two conditions for each normal distribution based upon the latent component
  if(z.i == 0) prices[i] <- rnorm(1, mean = 9, sd = 1)
  else prices[i] <- rnorm(1, mean = 20, sd = 1)
}
hist(prices)
```

# Motivation

-   Are the prices of books unimodal or bimodal?
-   Suppose you would want to predict the price of a book. Would its
    distribution be Normal or something else based on the the histogram.

# Motivation

```{r, echo=FALSE}
mu.pb   <- 9
sd.pb   <- 1
mu.hb   <- 20
sd.hb   <- 1

sample.pts <- seq(5, 25, by=0.1)
density_pb   <- dnorm(sample.pts, mean=mu.pb, sd=sd.pb)
density_hb <- dnorm(sample.pts, mean=mu.hb, sd=sd.hb)

plot(sample.pts, density_pb, col='red', type='l', xlab="Price ($)", ylab="Density", lty=2)
lines(sample.pts, density_hb, col='blue', type='l', lty=2)
lines(sample.pts, 0.5*density_hb + 0.5*density_pb, col='black', type='l', lwd=2)

legend('topright', c('paperback', 'hardback', 'all books'), col=c('red', 'blue', 'black'), lty=c(2,2,1), lwd=c(1,1,2), cex=0.7)
```

# Motivation

Now assume our data are the heights of students at university.

Male height: $N(69, 2.5^2),$ with units in inches.

Female height: $N(64, 2.5^2)$.

Assume that 75% of the population is female and 25% is male.

# Motivation

```{r, echo= FALSE}
numSamples <- 5000
heights      <- numeric(numSamples)
for(i in seq_len(numSamples)) {
  z.i <- rbinom(1,1,0.75)
  if(z.i == 0) heights[i] <- rnorm(1, mean = 69, sd = 2.5)
  else heights[i] <- rnorm(1, mean = 64, sd = 2.5)
}
hist(heights)
```

# Motivation

The histogram is now unimodal.

Are heights normally distributed (assuming this model)? Let's
invesitgate!

# Motivation

```{r, echo=FALSE}
mu.male   <- 69
sd.male   <- 2.5
mu.female <- 64
sd.female <- 2.5

sample.pts     <- seq(55, 80, by=0.1)
density_male   <- dnorm(sample.pts, mean=mu.male, sd=sd.male)
density_female <- dnorm(sample.pts, mean=mu.female, sd=sd.female)

plot(sample.pts, density_male, col='red', type='l', xlab="Height (inches)", ylab="Density", lty=2)
lines(sample.pts, density_female, col='blue', type='l', lty=2)
lines(sample.pts, .75*density_female + .25*density_male, col='black', type='l', lwd=2)

legend('topright', c('male', 'female', 'population'), col=c('red', 'blue', 'black'), lty=c(2,2,1), lwd=c(1,1,2), cex=0.7)
```

# Motivation

The Gaussian mixture model is unimodal because there is so much overlap
between the two densities.

In this example, observe that the population density is not symmetric,
and therefore not normally distributed.

# Goal

The goal of this module is to introduce **mixture models**, which are
commonly used in applications in classical and modern machine learning.

<!-- We will do this using a **latent variable**. -->

<!-- Background  -->

<!-- === -->

<!-- A **latent variable** is the true version of the state of a random variable that is unknown and not directly observed.\footnote{We will not delve into the properties of latent variables in this course.} -->

# Mixture models can be viewed as probabilistic clustering

-   Mixture models put similar data points into "clusters".

-   This is appealing as we can potentially compare different
    probabilistic clustering methods by how well they predict (under
    cross-validation). We will not explore this in this particular
    lecture.

-   This contrasts other methods such as k-means and hierarchical
    clustering as they produce clusters (and not predictions), so it's
    difficult to test if they are
    correct/incorrect.\footnote{Explore looking at these on your own and see if you can determine their limitations practically, compared to other machine learning models.}

# Two-component mixture model

Assume that both mixture components have the same precision,
$\lambda = 1/\sigma^2$, which is fixed and known. \vfill Let $\pi$ be
the mixture proportion for the first component. \vfill

Then the two-component Normal mixture model is: \begin{align}
  & X_1,\ldots, X_n \mid \mu,\pi\ \sim F(\mu,\pi)
\end{align} where $F(\mu,\pi)$ is the distribution with p.d.f.
$$ f(x|\mu,\textcolor{blue}{\pi}) = (1-\textcolor{blue}{\pi})\N(x\mid \mu_0,\lambda^{-1}) + \textcolor{blue}{\pi}
\N(x\mid \mu_1,\lambda^{-1}).$$

<!-- In the two-component mixture model, it assumes each observation is generated from one of two -->

<!--  mixture components, where $\pi$ is the mixture proportion for the first component and $1-\pi$ is the mixture proportion for the second component. -->

# Likelihood

The likelihood is \begin{align*}
    p(x_{1:n}|\mu,\pi) &= \prod_{i=1}^n f(x_i|\mu,\pi) \\
                       & = \prod_{i=1}^n \Big[ (1-\pi)\N(x_i\mid \mu_0,\lambda^{-1}) + \pi\N(x_i\mid \mu_1,\lambda^{-1}) \Big].
\end{align*}

# Likelihood

\textcolor{blue}{What do you notice about the likelihood function?}

```{=tex}
\begin{align*}
    p(x_{1:n}|\mu,\pi) &= \prod_{i=1}^n f(x_i|\mu,\pi) \\
                       & = \prod_{i=1}^n \Big[ (1-\pi)\N(x_i\mid \mu_0,\lambda^{-1}) + \pi\N(x_i\mid \mu_1,\lambda^{-1}) \Big].
\end{align*}
```
# Likelihood

The **likelihood** is very complicated function of $\mu$ and $\pi$.

\vspace*{1em}

This makes working with it directly to find the MLE (or other estimates)
difficult.

\vspace*{1em}

Thus, we will rewrite the likelihood using a two-stage approach.

# Two-stage approach

Let $Z_i$ indicate whether subject $i$ is from component 1 or 2.
\begin{align}
 & Z_1,\ldots,Z_n \,\stackrel{iid}{\sim}\,\Bernoulli(\pi) \\
 & X_i \mid  Z \sim\N(\mu_{Z_i},\lambda^{-1}) \quad i=1,\ldots,n. 
\end{align}

# Checking for understanding

Then the two-component Normal mixture model is: \begin{align}
  & X_1,\ldots, X_n \mid \mu,\pi\ \sim F(\mu,\pi)
\end{align} where $F(\mu,\pi)$ is the distribution with p.d.f.
$$ f(x|\mu,\textcolor{blue}{\pi}) = (1-\textcolor{blue}{\pi})\N(x\mid \mu_0,\lambda^{-1}) + \textcolor{blue}{\pi}
\N(x\mid \mu_1,\lambda^{-1}).$$

Written as a two-stage process:

```{=tex}
\begin{align}
 &Z_1,\ldots,Z_n \mid \mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
 & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) i=1,\ldots,n.
\end{align}
```
# Checking for understanding

Given the two equivalent models above, how would you simulate data from
a two component mixture model?

<!-- Equivalence of both models  -->

<!-- === -->

<!-- Recall -->

<!-- \begin{align*} -->

<!--  & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\ -->

<!--      & Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi) -->

<!-- \end{align*} -->

<!-- This is equivalent to the model above, since -->

<!-- \begin{align} -->

<!--     &p(x_i|\mu,\pi) \\ -->

<!--     &= p(x_i|Z_i=0,\mu,\pi)\Pr(Z_i=0|\mu,\pi) -->

<!--                     + p(x_i|Z_i=1,\mu,\pi)\Pr(Z_i=1|\mu,\pi) \\ -->

<!--             &= (1-\pi)\N(x_i|\mu_0,\lambda^{-1}) + \pi\N(x_i|\mu_1,\lambda^{-1})\\ -->

<!--             &= f(x_i|\mu,\pi), -->

<!-- \end{align} -->

<!-- and thus it induces the same distribution on $(x_{1:n},\mu,\pi)$. The latent model is considerably easier to work with mathematically!  -->

# Extension to k-components

Assume we observe $X_1,\ldots,X_n$ and that each $X_i$ is sampled from
one of $K$ **mixture components**.

Associated with each random variable $X_i$ is a label called
$Z_i \in \{1,\ldots,K\}$ which indicates which component $X_i$ came
from.

# Notation

Let $\pi_k$ be called **mixture proportions** or **mixture weights**,
which represent the probability that $X_i$ belongs to the $k$-th mixture
component. \vfill

The mixture proportions are non-negative and they sum to one,
$\sum_{k=1}^K \pi_k = 1$. \vfill

Observe that $P(X_i \mid Z_i=k)$ represents the distribution of $X_i$
assuming it came from component $k$.

# Extension

Then the two-component Normal mixture model is: \begin{align}
  & X_1,\ldots, X_n \mid \mu,\pi\ \sim F(\mu,\pi)
\end{align} where $F(\mu,\pi)$ is the distribution with p.d.f.
$$ f(x|\mu,\textcolor{blue}{\pi}) = \sum_{k=1}^K \pi_k N(\mu_k, \lambda^{-1}).$$

Written as a two-stage process: for $i=1,\ldots,n$: \begin{align}
& P(Z_i = k) = \pi_k \\
& X_i \mid \mu, Z_i \sim\N(\mu_{Z_i},\lambda^{-1}) 
\end{align}

# Example

Let's look at a three component mixture model.

Suppose we assume that $\mu = (-10, 0, 10)$ and $\sigma^2 = 1.$ Assume
each mixture weight is equally likely.

```{r}
set.seed(1234)
n <- 100
mu <- c(-10, 0, 10)
# sample Z first 
Z <- sample(1:3, size=n, replace=TRUE)
# conditional on Z, sample the normal update
X <- rnorm(n, mean=mu[Z], sd=1)
hist(X, breaks=20)
```

# Example

```{r, echo=FALSE}
n <- 100
mu <- c(-10, 0, 10)
# sample Z first 
Z <- sample(1:3, size=n, replace=TRUE)
# conditional on Z, sample the normal update
X <- rnorm(n, mean=mu[Z], sd=1)
hist(X, breaks=20)
```

<!-- Estimation -->

<!-- === -->

<!-- Now assume we are in the Gaussian mixture model setting where the $k$-th component is $N(\mu_k, \sigma^2)$ and the mixture proportions are $\pi_k$.  -->

<!-- How can we estimate $\{\mu_k,\sigma^2, \pi_k\}$ from the observed data $X_1,\ldots,X_n$?  -->

<!-- Solution: EM Algorithm.  -->

<!-- Conditional and marginal distributions -->

<!-- === -->

<!-- Recall that the conditional distribution $X_i|Z_i = k \sim N(\mu_k, \sigma_k^2),$ where $\pi_k = P(Z_i = k).$ -->

<!-- The marginal distribution of $X_i$ is: -->

<!-- \begin{align} -->

<!-- P(X_i = x) &= \sum_{k=1}^K P(Z_i = k) P(X_i=x | Z_i = k) \\ -->

<!-- &= \sum_{k=1}^K \pi_k N(x \mid \mu_k, \sigma_k^2) -->

<!-- \end{align} -->

<!-- Note: \textcolor{red}{$\sigma^2_k = \sigma^2$} moving forward. -->

<!-- Joint distribution -->

<!-- === -->

<!-- The joint probability of observations $X_1,\ldots,X_n$ is  -->

<!-- $$P(X_1=x_1,\ldots,X_n=x_n) = \prod_{i=1}^n \sum_{k=1}^K \pi_k N(x_i \mid \mu_k, \sigma_k^2)$$ -->

<!-- Exercise -->

<!-- === -->

<!-- Show that  -->

<!-- \begin{align} -->

<!-- &\log P(X_1, \ldots, X_n \mid \mu_1, \ldots, \mu_K) \\ -->

<!-- &= \log \prod_{i=1}^n P(x_i \mid \mu_1, \ldots, \mu_K) \\ -->

<!-- &= \sum_{i=1}^n \log [ -->

<!-- \sum_{k=1}^K P(x_i \mid \pi_k, \mu_1, \ldots, \mu_K) \pi_k -->

<!-- ] -->

<!-- \end{align} -->

<!-- Background -->

<!-- === -->

<!-- Recall that  -->

<!-- $$\frac{\partial \log f(x)}{\partial dx} = \frac{1}{f(x)}\frac{\partial f(x)}{\partial dx}.$$ -->

<!-- Exercise -->

<!-- === -->

<!-- Show that  -->

<!-- \begin{align} -->

<!-- &\frac{\partial \log P(X_1, \ldots, X_n \mid \mu_1, \ldots, \mu_K)}{\partial  \mu_k} \\ -->

<!-- &= \sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K) \frac{(x_i - \mu_k)}{\sigma} -->

<!-- \end{align} -->

<!-- This implies that  -->

<!-- $$\mu_k = \frac{\sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K) x_i} -->

<!-- {\sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K)},$$ -->

<!-- which is a non-linear equation of the $\mu_k$'s. -->

# EM Algorithm

General way to deal with hidden class labels or clusters. (Can also be
used for missing data or latent variables).

The E stands for “expectation”, because it gives us the conditional probabilities of different values of Z, and probabilities are expectations of indicator functions. The M stands for "“maximization."

# EM Algorithm

General way to deal with hidden class labels or clusters. (Can also be
used for missing data or latent variables).

E-step: Fill in the hidden class labels. 

M-step: Apply standard MLE (or other approaches) to complete the data.

The algorithm always converges to a local optima of the likelihood.

# Simple EM Algorithm

Notation and Setup
\vspace*{1em}

We know the following: 

-   Observations $x_{1:n}$.
-   K total classes
-   $P(Z_i = k) = \pi_k$ (for $i=1,\ldots, K$)
-   Common variance $\sigma^2.$

We do not know $\mu_1, \ldots, \mu_K$ and want to learn these. 

\vspace*{1em}

This is a very unrealistic setting, however, it hopefully provides intuition regarding the algorithm itself (and the math is simplified).

# EM Algorithm

$\propto$ will drop any constants (and I will make sure to include them back in later). Common trick in Bayesian statistics. 

```{=tex}
\begin{align}
& p(x_1, \ldots, x_n \mid \mu_1, \ldots, \mu_K) \\
& = \prod_{i=1}^n p(x_i \mid \mu_1, \ldots, \mu_K) \; \text{independent data}  \\
& = \prod_{i=1}^n \sum_{k=1}^K p(x_i, z_i = k \mid \mu_1, \ldots, \mu_K) \; \text{marg. over labels}\\
& = \prod_{i=1}^n \sum_{k=1}^K p(x_i \mid z_i = k, \mu_1, \ldots, \mu_K) p(z_i = k) \\
& \propto \prod_{i=1}^n \sum_{k=1}^K \exp(-\frac{1}{2\sigma^2} (x_i - \mu_k)^2) \pi_k \;\; \text{dropped normal constants}
\end{align}
```
# EM Algorithm

Let $\theta^{(t)} = (\mu_1^{(t)}, \ldots, \mu_k^{(t)})$ at some
iteration $t.$

At iteration $t$ consider the function:

```{=tex}
\begin{align}
Q(\theta^{(t)} \mid  \theta^{(t-1)}) &=
\sum_{i=1}^n \sum_{k=1}^K \textcolor{purple}{P(z_i = k \mid x_i, \theta^{(t-1)})} \\
& \quad \times \log P(x_i, z_i = k \mid \theta^{(t-1)})
\end{align}
```
# E-step

```{=tex}
\begin{align}
&\textcolor{purple}{P(z_i = k \mid x_i, \theta^{t-1})} \\
&= P(z_i = k \mid x_i, \mu_1^{(t-1)}, \ldots, \mu_K^{(t-1)}) \\
&\propto P(x_i \mid z_i = k, \mu_1^{(t-1)}, \ldots, \mu_K^{(t-1)}) P(z_i = k) \\
&\propto \exp(-\frac{1}{2\sigma^2} (x_i - \mu_k^{(t-1)})^2) \pi_k \\
&= \frac{\exp(-\frac{1}{2\sigma^2} (x_i - \mu_k^{(t-1)})^2) \pi_k}
{\sum_{k=1}^K \exp(-\frac{1}{2\sigma^2} (x_i - \mu_k^{(t-1)})^2) \pi_k}
\end{align}
```

This is equivalent to assigning clusters to each data point in a soft-way (clusters can overlap). 

# M-step

Recall that in the E-step, we calculated 
$R_{ik}^{(t-1)} = P(z_i = k \mid x_i, \theta^{(t-1)})$

\begin{align}
&Q(\theta^{(t)} \mid  \theta^{(t-1)}) \\
&=
\sum_{i=1}^n \sum_{k=1}^K \textcolor{purple}{P(z_i = k \mid x_i, \theta^{(t-1)})} \times
\log P(x_i, z_i = k \mid \theta^{(t-1)})\\
&= \sum_{i=1}^n \sum_{k=1}^K \textcolor{purple}{P(z_i = k \mid x_i, \theta^{(t-1)})} \\
& \quad \times
[
\log P(x_i \mid z_i = k, \theta^{(t-1)}) + \log P(z_i = k \mid \theta^{(t-1)})
] \\
&= \sum_{i=1}^n \sum_{k=1}^K
R_{ik}^{(t-1)} [
-\frac{1}{2\sigma^2} (x_i - \mu_k^{(t-1)})^2) + \log \pi_k
]
\end{align}


M-step
===

At each iteration $t$, maximize $Q$ in term of $\theta^{(t)}.$

\begin{align}
Q(\mu_k^{(t)} \mid \theta^{(t-1)}) &\propto \sum_{i=1}^n R_{ik}^{(t-1)} 
(-\frac{1}{2\sigma^2} (x_i - \mu_k^{(t-1)})^2), \implies \\
\frac{\partial Q(\mu_k^{(t)} \mid \theta^{(t-1)})}{\partial \mu_k^{(t)}} &= 
\sum_{i=1}^n R_{ik}^{(t-1)} (x_i - \mu_k^{(t-1)})) = 0 \implies
\end{align}

$$\textcolor{blue}{\mu_k^{(t)} = \sum_{i=1}^n w_i x_i} \quad \text{where}
$$

$$
w_i = \dfrac{
R_{ik}^{t-1}
}{
\sum_{i=1}^n R_{ik}^{t-1}
}
=
\dfrac{
P(z_i = k \mid x_i, \theta^{(t-1)})
}
{
\sum_{i=1}^n
P(z_i = k \mid x_i, \theta^{(t-1)})
}
$$


This is equivalent to updating the cluster centers. 

Summarize EM Algorithm
===

1. E-step

Compute the expected classes of all data points for each class:

$$P(z_i = k \mid x_i, \theta^{(t-1)}) =
\frac{\exp(-\frac{1}{2\sigma^2} (x_i - \mu_k^{(t-1)})^2) \pi_k^{(t-1)}}
{\sum_{k=1}^K \exp(-\frac{1}{2\sigma^2} (x_i - \mu_k^{(t-1)})^2) \pi_k^{(t-1)}}
$$

2. M-step

Then compute the maximum value given our data's class membership. 

$$\mu_i^{(t)} = \sum_{i=1}^n w_i x_i.$$ 

In this case, it's the MLE but with weighted data.

<!-- # Intuition of EM -->

<!-- $$\mu_k = \frac{\sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K) x_i} -->
<!-- {\sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K)},$$ -->

<!-- -   E-step: If for each $x_i$ we knew that for each $\pi_k$ the prob. -->
<!--     that $\mu_k$ was in component $\pi_k$ is -->
<!--     $P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K).$ Then we could compute -->
<!--     $\mu_k.$ -->

<!-- -   M-step: If we knew each $\mu_k$, then we could compute -->
<!--     $P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K)$ for each $\mu_k$ and $x_i$ -->

<!-- # EM Algorithm -->

<!-- Initalize all the unknown parameters. On iteration $t$, let the -->
<!-- estimates be $\lambda^{(t)} = \{\mu_1^{(t)}, \ldots, \mu_k^{(t)} \}$ -->

<!-- 1.  E-Step: -->

<!-- ```{=tex} -->
<!-- \begin{align} -->
<!-- P(\pi_k \mid x_i, \lambda^{(t)})  -->
<!-- &= -->
<!-- \frac{P(\pi_k \mid x_i, \lambda^{(t)}) x_i} -->
<!-- {P(\pi_k \mid x_i, \lambda^{(t)})} -->
<!-- \end{align} -->
<!-- ``` -->
<!-- 2.  M-Step: -->

<!-- ```{=tex} -->
<!-- \begin{align} -->
<!-- \mu_k^{(t+1)} &= \frac{\sum_{i=1}^n P(\pi_k \mid x_i,  \lambda^{(t)}) x_i} -->
<!-- {\sum_{i=1}^n P(\pi_k \mid x_i,  \lambda^{(t)})} -->
<!-- \end{align} -->
<!-- ``` -->
<!-- and $p_k^{(t+1)} = \frac{\sum_{i=1}^n P(\pi_k \mid x_i,  \lambda^{(t))}{n}$ -->

# Exercise

Assume our mixture components are fully specified Gaussian distributions
with $K=2$ components:

```{=tex}
\begin{align}
X_i \mid Z_i = 0 &\sim N(5, 1.5) \\
X_i \mid Z_i = 1 &\sim N(10, 2)
\end{align}
```
Let the true mixture proportions be $P(Z_i = 0) = 0.25$ and
$P(Z_i = 1) = 0.75$, respectively.

# Exercise

Simulate data from the mixture model on the previous slide, which
should produce the following histogram.

```{r, echo= FALSE}
# set mu and sigma
mu.true    = c(5, 10)
sigma.true = c(1.5, 2)

# Calculate Z
Z = rbinom(500, 1, 0.75)

# sample values from the mixture model 
myData <- rnorm(10000, mean=mu.true[Z+1], sd=sigma.true[Z+1])
hist(myData,breaks=15)
```



Exercise
===

```{r}
library(mixtools)
runEM <- normalmixEM(myData,  lambda = 0.5, 
                     mu = c(10, 20), sigma = c(2,2))
summary(runEM)
```
Plot
===

```{r}
plot(runEM)
```



# R packages for mixture models

-   The `mclust` package (<http://www.stat.washington.edu/mclust/>) is
    standard for Gaussian mixtures.

-   The `mixtools` considers classic parametric densities, mixtures of
    regressions, and some non-parametric mixtures.

# Exercise

Suppose that $X \sim N(\mu_1, \sigma_1^2)$ and
$Y \sim N(\mu_2, \sigma_2^2)$ independently.

1.  What is the distribution of $aX + bY$?

Solution: Due to independence,
$$Z \sim N(a\mu_1, + b\mu_2, a^2\sigma_1^2 +b^2 \sigma_2^2).$$

2.  Suppose that $X \sim N(\mu_1, \sigma_1^2)$ and
    $Y \sim N(\mu_2, \sigma_2^2)$ (and the observations are dependent).

Is the distribution of $aX + bY$ still Normal? No, not necessarily due
to the dependence of the random
variables.\footnote{In the case of a Gaussian mixture model, a random variable sampled from a Gaussian mixture model can be thought of as a two stage process. First, randomly sample a component (e.g., male or female). Second, then we sample our observation from the normal distribution that corresponds to the component sampled in step one.}

Other resources
===

1. Generalization to mixture models: https://github.com/resteorts/data-clean/blob/main/05-mixture-models/notes.pdf
2. Exercises: https://github.com/resteorts/data-clean/blob/main/exercises/exercise-intro-to-mixture-models.pdf
3. Notes: https://github.com/resteorts/data-clean/blob/main/05-mixture-models/notes.pdf