---
title: "Module 5: Fellegi-Sunter Method Applied to RLdata500"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---


 


## Load packages

Let's load packages. 

```{r, message=FALSE}
library(RecordLinkage)
```



## RLdata500

Let's consider the RLdata500 data set. 

```{r}
data(RLdata500)
#head(RLdata500)
#head(identity.RLdata500)
```

## Comparison Vectors

Why do we build record pairs of comparison vectors? 

\textbf{Answer}: Reduce the total number of record comparisons.

## Comparison Vectors

How do we build record pairs of comparison vectors?

\vspace*{1em}

\textbf{Answer}: Use the `compare.dedup` function. 

## Comparison Vectors


```{r}
# create comparison vectors
rpairs <- compare.dedup(RLdata500, 
                        identity = identity.RLdata500)
```

## Comparison Vectors 

\footnotesize
```{r}
# inspect comparison vectors
rpairs$pairs[1:5,]
```


## Blocking

Blocking is the reduction of the amount of data pairs through focusing on specified agreement patterns.

Blocking is a common strategy to reduce computation time and memory consumption by only comparing records with equal values for a subset of attributes, called blocking fields. 

## Blocking

A blocking specification can be supplied to the `compare` function via the argument `blockfld`. 

We will consider a blocking pattern where two records must agree in either the **first component of the first name** or **full date of birth**.

## Blocking and Comparison Vectors

```{r}
# blocking and comparison vectors
rpairs <- compare.dedup(RLdata500, 
                        blockfld = list(1,5:7),
                        identity = identity.RLdata500)
```

## Blocking and Comparison Vectors

\footnotesize
```{r}
# inspect comparison vectors
rpairs$pairs[c(1:3, 1203:1204),]
```


Observe that these records agree on first name but not date of birth (as designed).

## String Comparators

Recall that string comparators measure the similarity between strings, usually with a similarity measure in
the range $[0,1],$ where 0 denotes maximal dissimilarity and 1 equality.

Examples: Edit and Jaro Winkler 

## Blocking and String Comparators

```{r}
# blocking on birth day and month
# use jarowinkler string distance
rpairsfuzzy <- compare.dedup(RLdata500, 
                             blockfld = c(5,6), 
                             strcmp = TRUE,
                             strcmpfun = jarowinkler)

```

Blocking on **birth day and month** where the **Jaro Winkler** string comparator is used.  

## Blocking and String Comparators

\footnotesize
```{r}
# inspect first five record pairs 
rpairsfuzzy$pairs[1:5,]
```


## Probabilistic record linkage

Probabilistic record linkage relies on the assumption of
conditional probabilities concerning comparison patterns.

Recall that we defined the $u$ and $m$ probabilities previously as the following: 



$$u_{\tilde{\gamma}} = P(\gamma = \tilde{\gamma} \mid \text{the records are a match})$$

$$m_{\tilde{\gamma}} = P(\gamma = \tilde{\gamma} \mid \text{the records are not a match}).$$

## Probabilistic record linkage

The probabilities of the random vector $\gamma = (\gamma_1, \ldots \gamma_n)$
having value $\tilde{\gamma} = (\tilde{\gamma}_1, \ldots \tilde{\gamma}_n)$ conditional on the match status $Z$ can more precisely be defined as follows:



$$u_{\tilde{\gamma}} = P(\gamma = \tilde{\gamma} \mid Z=0)$$

$$m_{\tilde{\gamma}} = P(\gamma = \tilde{\gamma} \mid Z=1),$$

where $Z=0$ stands for a non-match and $Z=1$ stands for a match. 

## Probabilistic record linkage

In the Fellegi-Sunter model these probabilities are used to compute weights of the form

$$w_{\tilde{\gamma}} = \log \frac{P(\gamma = \tilde{\gamma} \mid Z=1)}{P(\gamma = \tilde{\gamma} \mid Z=0)}.$$

These weights are used in order to discern between matches and non-matches, where there are several ways to estimate the probabilities/weights.

## EM algorithm


The EM algorithm is used typically to estimate the weights, where the backbone of this algorithm is described by Haber (1984).

Weight calculation based on the EM algorithm and the method by Contiero et al. (2005) are implemented by functions `emWeights` and `epiWeights`.

Calling `summary` on the result shows the distribution of weights in histogram style. 

## EM algorithm

\footnotesize
```{r}
rpairs <- epiWeights(rpairs)
summary(rpairs)
```

## Computing Weight Thresholds

Discernment between matches and non-matches is achieved by means of computing weight thresholds.

The function `epiClassify` allows the user to specify a threshold. 

## Computing Weight Thresholds

\tiny
```{r}
result <- epiClassify(rpairs, 0.55)
summary(result)
head(result$prediction)
table(result$prediction)
head(identity.RLdata500)
```



## Error Measures
 
\tiny
```{r}
# recall and sensitivity are the same below
getErrorMeasures(result)
```

## Other objects

We can look at many objects of `result` which include

- data
- pairs
- frequencies
- type 
- Wdatas
- prediction
- threshold

 






