source("runFS.R")#
source("evaluate.R")#
set.seed(24052018)#
#
# -------------------------------- Load data --------------------------------- ##
# Read each file into a separate data frame#
fileDir <- "../datasets/"#
file.name <- "RLdata10000.csv"#
filesDf <- read.csv(paste0(fileDir,file.name), na.strings=c("NA"), #
                     stringsAsFactors = FALSE, colClasses = "character")#
#
# ------------------------------- Configuration ------------------------------ ##
linkingFields    <- c("fname_c1", "lname_c1", "by", "bm", "bd")#
strLinkingFields <- c("fname_c1", "lname_c1")#
blockPasses      <- list(c("by"), c("bm", "bd"))#
recIdField       <- "rec_id"#
entIdField       <- "ent_id"#
strDist          <- "levenshtein"#
strCutoff        <- 0.70#
dataName         <- "RLdata10000"#
threshold        <- -10.0#
sampleSize       <- c(5, 50) # This give 100 data points#
#sampleSize       <- c(5, 25)#
#
log <- file(paste0(dataName, ".log"), open = "wt")#
sink(log, type = "message")#
#
runFS(filesDf, linkingFields, strLinkingFields, recIdField, entIdField, blockPasses, #
      strDist, strCutoff, dataName, threshold)#
evaluate(filesDf, linkingFields, strLinkingFields, recIdField, #
         entIdField, blockPasses, strDist, strCutoff, dataName, #
         threshold, sampleSize)#
#
sink(type="message")#
#
# Uncomment to generate CSV file of candidate pairs#
#source("generatePairsCSV.R")#
#pairs.dataName <- "RLdata10000_pairs.csv"#
#generatePairsCSV(filesDf, linkingFields, strLinkingFields, recIdField, entIdField, #
#                 blockPasses, strDist, pairs.dataName)
# Runs Fellegi-Sunter based record linkage and saves the candidate links to #
# disk#
runFS <- function(filesDf, linkingFields, strLinkingFields, #
                  recIdField, entIdField, blockPasses, #
                  strDist, strCutoff, dataName, threshold) {#
  # Iterate over conjuctions (blocking passes)#
  for (bv in blockPasses) {#
    blockString <- paste(bv, collapse = ".")#
    message("*** Starting blocking pass \"", blockString, "\" ***")#
    # Get wall clock running time#
    t1 <- Sys.time()#
    # Index record pair comparisons (requires ~1.5GB free disk space)#
    # Note: we provide the ground truth cluster membership now, since this function #
    # is able to convert it to a pairwise format which we need for evaluation later #
    # on.#
    message("Indexing record pair comparisons...")#
    rpairs <- RLBigDataDedup(filesDf, #
                             blockfld = bv, #
                             identity = as.factor(filesDf[,entIdField]),#
                             exclude = union(setdiff(colnames(filesDf), linkingFields), bv), #
                             strcmp = strLinkingFields, #
                             strcmpfun = strDist)#
    vLinkingFields <- setdiff(colnames(rpairs@pairs), c("id1", "id2", "is_match"))#
    message("Verify linking fields: ", paste(vLinkingFields, collapse=", "), "\n")#
    # Extract the ground truth match status for the pairs, then remove from the #
    # rpairs object.#
    is_match <- clone.ff(rpairs@pairs$is_match)#
    numPairs <- length(is_match)#
    rpairs@pairs$is_match <- ffrep.int(as.integer(NA), numPairs)#
    # Run the EM algorithm to estimate the M and U probabilities#
    rpairs <- emWeights(rpairs, cutoff = strCutoff, verbose = TRUE)#
    #scores <- paste(as.ram(log2(rpairs@M/rpairs@U)), collapse = ", ")#
    #message("Scores for each pattern are [", scores, "]")#
    # Make predictions by applying a threhsold#
    result <- emClassify(rpairs, threshold.upper = threshold)#
    # Re-insert ground truth#
    result@data@pairs$is_match <- update(result@data@pairs$is_match, from = is_match)#
    # Get pairs above threshold in a data frame#
    resultDf <- getPairs(result, filter.link="link", single.rows = TRUE, #
                          withWeight = TRUE, withMatch = TRUE, withClass = FALSE)#
    csvPath <- paste0(dataName, "_candidates_", paste(bv, collapse = "."), ".csv")#
    message("Writing results to disk at ", csvPath)#
    write.table(resultDf[,c(paste0(recIdField,".", 1:2), "is_match", "Weight")], #
                file = csvPath, row.names = FALSE, col.names = FALSE, sep=",", quote = TRUE)#
    t2 <- Sys.time()#
    delta_t <- t2 - t1#
    message("Wall clock running time ", delta_t, " ", units(delta_t), "\n")#
    rm(rpairs, is_match, result, pred.match, pred.nonmatch, resultDf)#
    gc()#
  }#
}
setwd("~/work_on/teaching/almost-all-of-er/lectures/06-fellegi-sunter/fs-code")
source('~/work_on/teaching/almost-all-of-er/lectures/06-fellegi-sunter/fs-code/runFS.R', chdir = TRUE)
source("runFS.R")#
source("evaluate.R")#
set.seed(1234)#
#
# -------------------------------- Load data --------------------------------- ##
# Read each file into a separate data frame#
fileDir <- "../datasets/"#
file.name <- "RLdata10000.csv"#
filesDf <- read.csv(paste0(fileDir,file.name), na.strings=c("NA"), #
                     stringsAsFactors = FALSE, colClasses = "character")#
#
# ------------------------------- Configuration ------------------------------ ##
linkingFields    <- c("fname_c1", "lname_c1", "by", "bm", "bd")#
strLinkingFields <- c("fname_c1", "lname_c1")#
blockPasses      <- list(c("by"), c("bm", "bd"))#
recIdField       <- "rec_id"#
entIdField       <- "ent_id"#
strDist          <- "levenshtein"#
strCutoff        <- 0.70#
dataName         <- "RLdata10000"#
threshold        <- -10.0#
sampleSize       <- c(5, 50) # This give 100 data points#
#sampleSize       <- c(5, 25)#
#
log <- file(paste0(dataName, ".log"), open = "wt")#
sink(log, type = "message")#
#
runFS(filesDf, linkingFields, strLinkingFields, recIdField, entIdField, blockPasses, #
      strDist, strCutoff, dataName, threshold)#
evaluate(filesDf, linkingFields, strLinkingFields, recIdField, #
         entIdField, blockPasses, strDist, strCutoff, dataName, #
         threshold, sampleSize)#
#
sink(type="message")#
#
# Uncomment to generate CSV file of candidate pairs#
#source("generatePairsCSV.R")#
#pairs.dataName <- "RLdata10000_pairs.csv"#
#generatePairsCSV(filesDf, linkingFields, strLinkingFields, recIdField, entIdField, #
#                 blockPasses, strDist, pairs.dataName)
source("runFS.R")#
source("evaluate.R")#
library("ff")#
library("biglm")#
set.seed(1234)#
#
# -------------------------------- Load data --------------------------------- ##
# Read each file into a separate data frame#
fileDir <- "../datasets/"#
file.name <- "RLdata10000.csv"#
filesDf <- read.csv(paste0(fileDir,file.name), na.strings=c("NA"), #
                     stringsAsFactors = FALSE, colClasses = "character")#
#
# ------------------------------- Configuration ------------------------------ ##
linkingFields    <- c("fname_c1", "lname_c1", "by", "bm", "bd")#
strLinkingFields <- c("fname_c1", "lname_c1")#
blockPasses      <- list(c("by"), c("bd"))#
recIdField       <- "rec_id"#
entIdField       <- "ent_id"#
strDist          <- "levenshtein"#
strCutoff        <- 0.70#
dataName         <- "RLdata10000"#
threshold        <- -10.0#
sampleSize       <- c(5, 50) # This give 100 data points#
#sampleSize       <- c(5, 25)#
#
log <- file(paste0(dataName, ".log"), open = "wt")#
sink(log, type = "message")#
#
runFS(filesDf, linkingFields, strLinkingFields, recIdField, entIdField, blockPasses, #
      strDist, strCutoff, dataName, threshold)#
evaluate(filesDf, linkingFields, strLinkingFields, recIdField, #
         entIdField, blockPasses, strDist, strCutoff, dataName, #
         threshold, sampleSize)#
#
sink(type="message")
require(RecordLinkage)#
require(ffbase)
library(blink)library(knitr)library(textreuse) # text reuse/document similaritylibrary(tokenizers) # shingleslibrary(devtools)library(cora)library(ggplot2)# install_github("resteorts/cora")data(cora) # load the cora data setdata(cora_gold) head(cora_gold_update) # contains the id and the unique idtail(cora_gold_update)dim(cora_gold_update)
runFS <- function(filesDf, linkingFields, strLinkingFields, #
                  recIdField, entIdField, blockPasses, #
                  strDist, strCutoff, dataName, threshold) {#
  # Iterate over conjuctions (blocking passes)#
  for (bv in blockPasses) {#
    blockString <- paste(bv, collapse = ".")#
    message("*** Starting blocking pass \"", blockString, "\" ***")#
    # Get wall clock running time#
    t1 <- Sys.time()#
    # Index record pair comparisons (requires ~1.5GB free disk space)#
    # Note: we provide the ground truth cluster membership now, since this function #
    # is able to convert it to a pairwise format which we need for evaluation later #
    # on.#
    message("Indexing record pair comparisons...")#
    rpairs <- RLBigDataDedup(filesDf, #
                             blockfld = bv, #
                             identity = as.factor(filesDf[,entIdField]),#
                             exclude = union(setdiff(colnames(filesDf), linkingFields), bv), #
                             strcmp = strLinkingFields, #
                             strcmpfun = strDist)#
    vLinkingFields <- setdiff(colnames(rpairs@pairs), c("id1", "id2", "is_match"))#
    message("Verify linking fields: ", paste(vLinkingFields, collapse=", "), "\n")#
    # Extract the ground truth match status for the pairs, then remove from the #
    # rpairs object.#
    is_match <- clone.ff(rpairs@pairs$is_match)#
    numPairs <- length(is_match)#
    rpairs@pairs$is_match <- ffrep.int(as.integer(NA), numPairs)#
    # Run the EM algorithm to estimate the M and U probabilities#
    rpairs <- emWeights(rpairs, cutoff = strCutoff, verbose = TRUE)#
    #scores <- paste(as.ram(log2(rpairs@M/rpairs@U)), collapse = ", ")#
    #message("Scores for each pattern are [", scores, "]")#
    # Make predictions by applying a threhsold#
    result <- emClassify(rpairs, threshold.upper = threshold)#
    # Re-insert ground truth#
    result@data@pairs$is_match <- update(result@data@pairs$is_match, from = is_match)#
    # Get pairs above threshold in a data frame#
    resultDf <- getPairs(result, filter.link="link", single.rows = TRUE, #
                          withWeight = TRUE, withMatch = TRUE, withClass = FALSE)#
    csvPath <- paste0(dataName, "_candidates_", paste(bv, collapse = "."), ".csv")#
    message("Writing results to disk at ", csvPath)#
    write.table(resultDf[,c(paste0(recIdField,".", 1:2), "is_match", "Weight")], #
                file = csvPath, row.names = FALSE, col.names = FALSE, sep=",", quote = TRUE)#
    t2 <- Sys.time()#
    delta_t <- t2 - t1#
    message("Wall clock running time ", delta_t, " ", units(delta_t), "\n")#
    rm(rpairs, is_match, result, pred.match, pred.nonmatch, resultDf)#
    gc()#
  }#
}
getwd()
source("runFS.R")#
source("evaluate.R")#
set.seed(24052018)
fileDir <- "../datasets/"#
file.name <- "RLdata10000.csv"#
filesDf <- read.csv(paste0(fileDir,file.name), na.strings=c("NA"), #
                     stringsAsFactors = FALSE, colClasses = "character")
linkingFields    <- c("fname_c1", "lname_c1", "by", "bm", "bd")#
strLinkingFields <- c("fname_c1", "lname_c1")#
blockPasses      <- list(c("by"), c("bm", "bd"))#
recIdField       <- "rec_id"#
entIdField       <- "ent_id"#
strDist          <- "levenshtein"#
strCutoff        <- 0.70#
dataName         <- "RLdata10000"#
threshold        <- -10.0#
sampleSize       <- c(5, 50) # This give 100 data points#
#sampleSize       <- c(5, 25)
log <- file(paste0(dataName, ".log"), open = "wt")#
sink(log, type = "message")#
#
runFS(filesDf, linkingFields, strLinkingFields, recIdField, entIdField, blockPasses, #
      strDist, strCutoff, dataName, threshold)#
evaluate(filesDf, linkingFields, strLinkingFields, recIdField, #
         entIdField, blockPasses, strDist, strCutoff, dataName, #
         threshold, sampleSize)#
#
sink(type="message")
